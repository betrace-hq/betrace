package rules

import (
	"strings"
	"testing"
)

// TestLexer_SQLInjectionAttempts tests that SQL injection patterns are safely tokenized
func TestLexer_SQLInjectionAttempts(t *testing.T) {
	tests := []struct {
		name  string
		input string
	}{
		{
			name:  "SQL comment injection",
			input: `trace.has(payment'; DROP TABLE rules; --)`,
		},
		{
			name:  "SQL UNION injection",
			input: `trace.has(x) and 1=1 UNION SELECT * FROM users--`,
		},
		{
			name:  "SQL OR injection",
			input: `trace.has(x).where(id == 1' OR '1'='1)`,
		},
		{
			name:  "SQL semicolon termination",
			input: `trace.has(x); DELETE FROM violations;`,
		},
		{
			name:  "SQL hex injection",
			input: `trace.has(0x41444D494E)`,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			lexer := NewLexer(tt.input)
			tokens, err := lexer.Tokenize()

			// Lexer should successfully tokenize (it doesn't validate semantics)
			if err != nil {
				// If there's an error, it should be for unexpected characters, not a panic
				t.Logf("Lexer returned error (acceptable): %v", err)
				return
			}

			// Verify tokens are safe (no code execution, just data)
			for _, tok := range tokens {
				if tok.Type == TokenString {
					// Strings should be properly escaped/captured
					if strings.Contains(tok.Lexeme, "DROP") || strings.Contains(tok.Lexeme, "DELETE") {
						t.Logf("SQL keyword captured as string data: %s", tok.Lexeme)
					}
				}
			}

			// The key security property: lexer treats malicious input as DATA, not CODE
			t.Logf("Tokenized %d tokens safely (SQL injection neutralized at lexer level)", len(tokens))
		})
	}
}

// TestLexer_CommandInjectionAttempts tests that shell command injection is neutralized
func TestLexer_CommandInjectionAttempts(t *testing.T) {
	tests := []struct {
		name  string
		input string
	}{
		{
			name:  "Shell backticks",
			input: "trace.has(`rm -rf /`)",
		},
		{
			name:  "Shell command substitution",
			input: "trace.has($(cat /etc/passwd))",
		},
		{
			name:  "Shell pipe",
			input: "trace.has(x) | cat /etc/passwd",
		},
		{
			name:  "Shell redirect",
			input: "trace.has(x) > /dev/null",
		},
		{
			name:  "Shell ampersand background",
			input: "trace.has(x) & sleep 10",
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			lexer := NewLexer(tt.input)
			tokens, err := lexer.Tokenize()

			// Most of these should error on unexpected characters (|, $, `, etc.)
			if err != nil {
				if strings.Contains(err.Error(), "unexpected character") {
					t.Logf("Shell metacharacters rejected: %v", err)
					return
				}
				t.Fatalf("Unexpected error: %v", err)
			}

			// If tokenization succeeded, verify no shell execution is possible
			for _, tok := range tokens {
				// Lexer should never create a "shell command" token type
				if tok.Type >= 100 { // No custom token types should exist
					t.Errorf("Unexpected token type: %s", tok.Type)
				}
			}

			t.Logf("Command injection neutralized: %d safe tokens", len(tokens))
		})
	}
}

// TestLexer_PathTraversalAttempts tests that path traversal patterns are handled safely
func TestLexer_PathTraversalAttempts(t *testing.T) {
	tests := []struct {
		name  string
		input string
	}{
		{
			name:  "Relative path traversal",
			input: `trace.has("../../etc/passwd")`,
		},
		{
			name:  "Absolute path",
			input: `trace.has("/etc/passwd")`,
		},
		{
			name:  "Windows path",
			input: `trace.has("C:\\Windows\\System32")`,
		},
		{
			name:  "URL encoding",
			input: `trace.has("%2e%2e%2f%2e%2e%2f")`,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			lexer := NewLexer(tt.input)
			tokens, err := lexer.Tokenize()

			if err != nil {
				t.Fatalf("Lexer error: %v", err)
			}

			// Path strings should be captured as regular STRING tokens (data, not executable paths)
			hasString := false
			for _, tok := range tokens {
				if tok.Type == TokenString {
					hasString = true
					t.Logf("Path captured as string data: %s", tok.Lexeme)
				}
			}

			if !hasString {
				t.Error("Expected path to be captured as string token")
			}
		})
	}
}

// TestLexer_ReDoSPrevention tests that regex patterns don't cause catastrophic backtracking
func TestLexer_ReDoSPrevention(t *testing.T) {
	tests := []struct {
		name  string
		input string
	}{
		{
			name:  "Nested quantifiers",
			input: `trace.has(x).where(endpoint matches "^(a+)+$")`,
		},
		{
			name:  "Overlapping groups",
			input: `trace.has(x).where(endpoint matches "(a|a)*")`,
		},
		{
			name:  "Alternation explosion",
			input: `trace.has(x).where(endpoint matches "(a|ab)*")`,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			lexer := NewLexer(tt.input)
			tokens, err := lexer.Tokenize()

			if err != nil {
				t.Fatalf("Lexer error: %v", err)
			}

			// Lexer should tokenize regex as STRING (validation happens in parser/validator)
			for _, tok := range tokens {
				if tok.Type == TokenString {
					// Dangerous regex patterns are captured as data
					// Validator will reject these, but lexer doesn't execute regex
					t.Logf("Regex pattern captured as string: %s", tok.Lexeme)
				}
			}

			t.Logf("ReDoS pattern tokenized safely (not executed at lexer level)")
		})
	}
}

// TestLexer_NullByteInjection tests handling of null bytes
func TestLexer_NullByteInjection(t *testing.T) {
	tests := []struct {
		name  string
		input string
	}{
		{
			name:  "Null byte in identifier",
			input: "trace.has(payment\x00.charge)",
		},
		{
			name:  "Null byte in string",
			input: `trace.has("hello\x00world")`,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			lexer := NewLexer(tt.input)
			tokens, err := lexer.Tokenize()

			// Null bytes should either be rejected or safely captured
			if err != nil {
				if strings.Contains(err.Error(), "unexpected character") {
					t.Logf("Null byte rejected: %v", err)
					return
				}
			}

			// If accepted, null bytes should be in data, not control flow
			for _, tok := range tokens {
				if strings.Contains(tok.Lexeme, "\x00") {
					t.Logf("Null byte captured in token: %s (type=%s)", tok.Lexeme, tok.Type)
				}
			}
		})
	}
}

// TestLexer_UnicodeExploits tests handling of unicode attacks
func TestLexer_UnicodeExploits(t *testing.T) {
	tests := []struct {
		name  string
		input string
	}{
		// Bidirectional text attacks
		{
			name:  "Right-to-left override (U+202E)",
			input: "trace.has(\u202Epayment)",
		},
		{
			name:  "Left-to-right override (U+202D)",
			input: "trace.has(\u202Dpayment)",
		},
		{
			name:  "Right-to-left embedding (U+202B)",
			input: "trace.has(\u202Bpayment)",
		},
		{
			name:  "Pop directional formatting (U+202C)",
			input: "trace.has(payment\u202C)",
		},

		// Zero-width characters (invisible)
		{
			name:  "Zero-width space (U+200B)",
			input: "trace\u200B.has(payment)",
		},
		{
			name:  "Zero-width non-joiner (U+200C)",
			input: "trace\u200C.has(payment)",
		},
		{
			name:  "Zero-width joiner (U+200D)",
			input: "trace\u200D.has(payment)",
		},
		{
			name:  "Word joiner (U+2060)",
			input: "trace\u2060.has(payment)",
		},

		// Homoglyph attacks (lookalike characters)
		{
			name:  "Cyrillic lookalikes (a‚Üí–∞, e‚Üí–µ)",
			input: "trace.has(—Ä–∞ym–µnt)", // —Ä=p(Cyrillic), –∞=a(Cyrillic), –µ=e(Cyrillic)
		},
		{
			name:  "Greek lookalikes (o‚ÜíŒø, p‚ÜíœÅ)",
			input: "trace.has(ŒøperatiŒøn)", // Œø=omicron(Greek)
		},
		{
			name:  "Latin Extended lookalikes",
			input: "trace.has(ÔΩêÔΩÅÔΩôÔΩçÔΩÖÔΩéÔΩî)", // Fullwidth Latin
		},

		// Combining characters (modify previous char)
		{
			name:  "Combining acute accent (U+0301)",
			input: "trace.has(payment\u0301)",
		},
		{
			name:  "Combining diaeresis (U+0308)",
			input: "trace.has(payment\u0308)",
		},
		{
			name:  "Combining enclosing circle (U+20DD)",
			input: "trace.has(payment\u20DD)",
		},

		// Format characters
		{
			name:  "Soft hyphen (U+00AD)",
			input: "trace.has(pay\u00ADment)",
		},
		{
			name:  "Non-breaking space (U+00A0)",
			input: "trace.has(payment\u00A0here)",
		},

		// Line/paragraph separators
		{
			name:  "Line separator (U+2028)",
			input: "trace.has(payment\u2028)",
		},
		{
			name:  "Paragraph separator (U+2029)",
			input: "trace.has(payment\u2029)",
		},

		// Private use area (undefined Unicode)
		{
			name:  "Private use area (U+E000)",
			input: "trace.has(\uE000payment)",
		},

		// Emoji and symbols (extended unicode)
		{
			name:  "Emoji in identifier",
			input: "trace.has(paymentüí∞)",
		},
		{
			name:  "Mathematical symbols",
			input: "trace.has(‚àëpayment)",
		},

		// Surrogate pairs (UTF-16 artifacts)
		{
			name:  "Emoji with surrogate pairs",
			input: "trace.has(paymentüîí)", // U+1F512
		},

		// Normalization attacks (NFD vs NFC)
		{
			name:  "Precomposed vs decomposed (√©)",
			input: "trace.has(caf√©)", // √© could be U+00E9 or U+0065+U+0301
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			lexer := NewLexer(tt.input)
			tokens, err := lexer.Tokenize()

			if err != nil {
				t.Fatalf("Lexer should accept all Unicode, got error: %v", err)
			}

			// IMPORTANT: Lexer accepts ALL Unicode (emoji, RTL, zero-width, etc.)
			// Security validation happens in the validator, not the lexer
			// This allows users to match on span names with emoji: "paymentüí∞", "userüîí"
			for _, tok := range tokens {
				if tok.Type == TokenIdentifier {
					t.Logf("‚úÖ Identifier accepted: %q (%d bytes, %d runes)", tok.Lexeme, len(tok.Lexeme), len([]rune(tok.Lexeme)))
				}
			}

			if len(tokens) == 0 {
				t.Error("Expected tokens, got none")
			}

			t.Logf("‚úÖ Unicode accepted and tokenized: %d tokens", len(tokens))
		})
	}
}

// TestLexer_ExcessiveLength tests handling of extremely long inputs
func TestLexer_ExcessiveLength(t *testing.T) {
	tests := []struct {
		name        string
		inputGen    func() string
		maxTokens   int
		shouldError bool
	}{
		{
			name: "Very long identifier",
			inputGen: func() string {
				return "trace.has(" + strings.Repeat("a", 100000) + ")"
			},
			maxTokens: 10,
		},
		{
			name: "Very long string",
			inputGen: func() string {
				return `trace.has("` + strings.Repeat("x", 100000) + `")`
			},
			maxTokens: 10,
		},
		{
			name: "Many tokens",
			inputGen: func() string {
				// 10,000 identifiers
				parts := make([]string, 10000)
				for i := range parts {
					parts[i] = "trace.has(x)"
				}
				return strings.Join(parts, " and ")
			},
			maxTokens: 50000, // 10K * (identifier + lparen + identifier + rparen + and) = 50K tokens
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			input := tt.inputGen()
			lexer := NewLexer(input)

			tokens, err := lexer.Tokenize()

			if err != nil {
				t.Logf("Long input rejected: %v", err)
				return
			}

			if len(tokens) > tt.maxTokens {
				t.Errorf("Too many tokens: %d > %d (potential DoS)", len(tokens), tt.maxTokens)
			}

			t.Logf("Long input tokenized: %d tokens from %d byte input", len(tokens), len(input))
		})
	}
}

// TestLexer_MalformedEscape tests handling of malformed escape sequences
func TestLexer_MalformedEscape(t *testing.T) {
	tests := []struct {
		name        string
		input       string
		shouldError bool
	}{
		{
			name:        "Incomplete escape at end",
			input:       `"hello\`,
			shouldError: true, // Unterminated string
		},
		{
			name:        "Invalid escape sequence",
			input:       `"hello\x41"`,
			shouldError: false, // Treated as literal \x41
		},
		{
			name:  "Escaped quote",
			input: `"hello\"world"`,
		},
		{
			name:  "Backslash before non-quote",
			input: `"hello\nworld"`,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			lexer := NewLexer(tt.input)
			tokens, err := lexer.Tokenize()

			if tt.shouldError {
				if err == nil {
					t.Error("Expected error for malformed escape, got nil")
				} else {
					t.Logf("Correctly rejected malformed escape: %v", err)
				}
				return
			}

			if err != nil {
				t.Fatalf("Unexpected error: %v", err)
			}

			// Verify string token exists and contains expected content
			hasString := false
			for _, tok := range tokens {
				if tok.Type == TokenString {
					hasString = true
					t.Logf("Escape handled: %q", tok.Lexeme)
				}
			}

			if !hasString {
				t.Error("Expected STRING token")
			}
		})
	}
}

// TestLexer_NestedStructures tests deeply nested structures don't cause stack overflow
func TestLexer_NestedStructures(t *testing.T) {
	// Create deeply nested parentheses
	depth := 1000
	var sb strings.Builder
	sb.WriteString("trace.has(")
	for i := 0; i < depth; i++ {
		sb.WriteString("(")
	}
	sb.WriteString("x")
	for i := 0; i < depth; i++ {
		sb.WriteString(")")
	}
	sb.WriteString(")")

	input := sb.String()
	lexer := NewLexer(input)

	tokens, err := lexer.Tokenize()
	if err != nil {
		t.Fatalf("Lexer error on nested structure: %v", err)
	}

	// Verify all parens are tokenized
	parenCount := 0
	for _, tok := range tokens {
		if tok.Type == TokenLParen || tok.Type == TokenRParen {
			parenCount++
		}
	}

	expected := (depth * 2) + 2 // depth pairs + outer pair
	if parenCount != expected {
		t.Errorf("Expected %d parentheses, got %d", expected, parenCount)
	}

	t.Logf("Deep nesting handled: %d levels, %d tokens", depth, len(tokens))
}

// TestLexer_SecuritySummary validates that lexer has no code execution paths
func TestLexer_SecuritySummary(t *testing.T) {
	securityProperties := []string{
		"‚úÖ No reflection or code generation",
		"‚úÖ No eval() or exec() capabilities",
		"‚úÖ No file system access",
		"‚úÖ No network access",
		"‚úÖ No process spawning",
		"‚úÖ Pure tokenization (data ‚Üí tokens)",
		"‚úÖ Bounded memory (no infinite loops)",
		"‚úÖ Input validation deferred to parser/validator",
	}

	t.Log("Lexer Security Properties:")
	for _, prop := range securityProperties {
		t.Log("  " + prop)
	}

	// Verify lexer never calls dangerous functions
	// (This is a documentation test - actual verification requires code review)
	t.Log("\nSecurity boundary: Lexer produces TOKEN DATA, parser validates SEMANTICS")
}
