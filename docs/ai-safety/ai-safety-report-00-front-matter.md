# International Scientific Report on the Safety of Advanced AI
## January 2025

---

## Contributors

### CHAIR
Prof. Yoshua Bengio, Université de Montréal / Mila - Quebec AI Institute

### EXPERT ADVISORY PANEL
This international panel was nominated by the governments of the 30 countries listed below, the UN, EU, and OECD.

- **Australia**: Bronwyn Fox, the University of New South Wales
- **Brazil**: André Carlos Ponce de Leon Ferreira de Carvalho, Institute of Mathematics and Computer Sciences, University of São Paulo
- **Canada**: Mona Nemer, Chief Science Advisor of Canada
- **Chile**: Raquel Pezoa Rivera, Universidad Técnica Federico Santa Maria
- **China**: Yi Zeng, Chinese Academy of Sciences
- **European Union**: Juha Heikkilä, European AI Office
- **France**: Guillaume Avrin, National Coordination for Artificial Intelligence
- **Germany**: Antonio Krüger, German Research Center for Artificial Intelligence
- **India**: Balaraman Ravindran, Wadhwani School of Data Science and AI, Indian Institute of Technology Madras
- **Indonesia**: Hammam Riza, Collaborative Research and Industrial Innovation in Artificial Intelligence (KORIKA)
- **Ireland**: Ciarán Seoighe, Research Ireland
- **Israel**: Ziv Katzir, Israel Innovation Authority
- **Italy**: Andrea Monti, Legal Expert for the Undersecretary of State for the Digital Transformation, Italian Ministers Council's Presidency
- **Japan**: Hiroaki Kitano, Sony Group Corporation
- **Kenya**: Nusu Mwamanzi, Ministry of ICT & Digital Economy
- **Kingdom of Saudi Arabia**: Fahad Albalawi, Saudi Authority for Data and Artificial Intelligence
- **Mexico**: José Ramón López Portillo, LobsterTel
- **Netherlands**: Haroon Sheikh, Netherlands' Scientific Council for Government Policy
- **New Zealand**: Gill Jolly, Ministry of Business, Innovation and Employment
- **Nigeria**: Olubunmi Ajala, Ministry of Communications, Innovation and Digital Economy
- **OECD**: Jerry Sheehan, Director of the Directorate for Science, Technology and Innovation
- **Philippines**: Dominic Vincent Ligot, CirroLytix
- **Republic of Korea**: Kyoung Mu Lee, Department of Electrical and Computer Engineering, Seoul National University
- **Rwanda**: Crystal Rugege, Centre for the Fourth Industrial Revolution
- **Singapore**: Denise Wong, Data Innovation and Protection Group, Infocomm Media Development Authority
- **Spain**: Nuria Oliver, ELLIS Alicante
- **Switzerland**: Christian Busch, Federal Department of Economic Affairs, Education and Research
- **Türkiye**: Ahmet Halit Hatip, Turkish Ministry of Industry and Technology
- **Ukraine**: Oleksii Molchanovskyi, Expert Committee on the Development of Artificial Intelligence in Ukraine
- **United Arab Emirates**: Marwan Alserkal, Ministry of Cabinet Affairs, Prime Minister's Office
- **United Kingdom**: Chris Johnson, Chief Scientific Adviser in the Department for Science, Innovation and Technology
- **United Nations**: Amandeep Singh Gill, Under-Secretary-General for Digital and Emerging Technologies and Secretary-General's Envoy on Technology
- **United States**: Saif M. Khan, U.S. Department of Commerce

### SCIENTIFIC LEAD
Sören Mindermann, Mila - Quebec AI Institute

### LEAD WRITER
Daniel Privitera, KIRA Center

### WRITING GROUP
- Tamay Besiroglu, Epoch AI
- Rishi Bommasani, Stanford University
- Stephen Casper, Massachusetts Institute of Technology
- Yejin Choi, Stanford University
- Philip Fox, KIRA Center
- Ben Garfinkel, University of Oxford
- Danielle Goldfarb, Mila - Quebec AI Institute
- Hoda Heidari, Carnegie Mellon University
- Anson Ho, Epoch AI
- Sayash Kapoor, Princeton University
- Leila Khalatbari, Hong Kong University of Science and Technology
- Shayne Longpre, Massachusetts Institute of Technology
- Sam Manning, Centre for the Governance of AI
- Vasilios Mavroudis, The Alan Turing Institute
- Mantas Mazeika, University of Illinois at Urbana-Champaign
- Julian Michael, New York University
- Jessica Newman, University of California, Berkeley
- Kwan Yee Ng, Concordia AI
- Chinasa T. Okolo, Brookings Institution
- Deborah Raji, University of California, Berkeley
- Girish Sastry, Independent
- Elizabeth Seger (generalist writer), Demos
- Theodora Skeadas, Humane Intelligence
- Tobin South, Massachusetts Institute of Technology

### SENIOR ADVISERS
- Daron Acemoglu, Massachusetts Institute of Technology
- Olubayo Adekanmbi, contributed as a Senior Adviser prior to taking up his role at EqualyzAI
- David Dalrymple, Advanced Research + Invention Agency
- Thomas G. Dietterich, Oregon State University
- Edward W. Felten, Princeton University
- Pascale Fung, contributed as a Senior Adviser prior to taking up her role at Meta
- Pierre-Olivier Gourinchas, Research Department, International Monetary Fund
- Fredrik Heintz, Linköping University
- Geoffrey Hinton, University of Toronto
- Nick Jennings, University of Loughborough
- Andreas Krause, ETH Zurich
- Susan Leavy, University College Dublin
- Percy Liang, Stanford University
- Teresa Ludermir, Federal University of Pernambuco
- Vidushi Marda, AI Collaborative
- Emma Strubell, Carnegie Mellon University
- Florian Tramèr, ETH Zurich
- Lucia Velasco, Maastricht University
- Nicole Wheeler, University of Birmingham
- Helen Margetts, University of Oxford
- John McDermid, University of York
- Jane Munga, Carnegie Endowment for International Peace
- Arvind Narayanan, Princeton University
- Alondra Nelson, Institute for Advanced Study
- Clara Neppel, IEEE
- Alice Oh, KAIST School of Computing
- Gopal Ramchurn, Responsible AI UK
- Stuart Russell, University of California, Berkeley
- Marietje Schaake, Stanford University
- Bernhard Schölkopf, ELLIS Institute Tübingen
- Dawn Song, University of California, Berkeley
- Alvaro Soto, Pontificia Universidad Católica de Chile
- Lee Tiedrich, Duke University
- Gaël Varoquaux, Inria
- Andrew Yao, Institute for Interdisciplinary Information Sciences, Tsinghua University
- Ya-Qin Zhang, Tsinghua University

### SECRETARIAT

**AI Safety Institute**
- Baran Acar
- Ben Clifford
- Lambrini Das
- Claire Dennis
- Freya Hempleman
- Hannah Merchant
- Rian Overy
- Ben Snodin

**Mila — Quebec AI Institute**
- Jonathan Barry
- Benjamin Prud'homme

---

## Acknowledgements

### Civil Society and Industry Reviewers

**Civil Society**: Ada Lovelace Institute, AI Forum New Zealand / Te Kāhui Atamai Iahiko o Aotearoa, Australia's Temporary AI Expert Group, Carnegie Endowment for International Peace, Center for Law and Innovation / Certa Foundation, Centre for the Governance of AI, Chief Justice Meir Shamgar Center for Digital Law and Innovation, Eon Institute, Gradient Institute, Israel Democracy Institute, Mozilla Foundation, Old Ways New, RAND, SaferAI, The Centre for Long-Term Resilience, The Future Society, The Alan Turing Institute, The Royal Society, Türkiye Artificial Intelligence Policies Association.

**Industry**: Advai, Anthropic, Cohere, Deloitte Consulting USA and Deloitte LLM UK, G42, Google DeepMind, Harmony Intelligence, Hugging Face, IBM, Lelapa AI, Meta, Microsoft, Shutterstock, Zhipu.ai.

### Special Thanks
The Secretariat appreciates the support, comments and feedback from Angie Abdilla, Concordia AI, Nitarshan Rajkumar, Geoffrey Irving, Shannon Vallor, Rebecca Finlay and Andrew Strait.

---

## License and Copyright

© Crown owned 2025

This publication is licensed under the terms of the Open Government Licence v3.0 except where otherwise stated. To view this licence, visit https://www.nationalarchives.gov.uk/doc/opengovernment-licence/version/3/ or write to the Information Policy Team, The National Archives, Kew, London TW9 4DU, or email: psi@nationalarchives.gsi.gov.uk.

Where we have identified any third-party copyright information you will need to obtain permission from the copyright holders concerned.

Any enquiries regarding this publication should be sent to: secretariat.AIStateofScience@dsit.gov.uk.

Enquiries regarding the content of the report should also be sent to the Scientific Lead.

### Disclaimer
The report does not represent the views of the Chair, any particular individual in the writing or advisory groups, nor any of the governments that have supported its development. This report is a synthesis of the existing research on the capabilities and risks of advanced AI. The Chair of the report has ultimate responsibility for it and has overseen its development from beginning to end.

**Research series number**: DSIT 2025/001

---

## Forewords

### Building a shared scientific understanding in a fast-moving field
**Professor Yoshua Bengio, Université de Montréal / Mila – Quebec AI Institute & Chair**

I am honoured to present the International AI Safety Report. It is the work of 96 international AI experts who collaborated in an unprecedented effort to establish an internationally shared scientific understanding of risks from advanced AI and methods for managing them.

We embarked on this journey just over a year ago, shortly after the countries present at the Bletchley Park AI Safety Summit agreed to support the creation of this report. Since then, we published an Interim Report in May 2024, which was presented at the AI Seoul Summit. We are now pleased to publish the present, full report ahead of the AI Action Summit in Paris in February 2025.

Since the Bletchley Summit, the capabilities of general-purpose AI, the type of AI this report focuses on, have increased further. For example, new models have shown markedly better performance at tests of programming and scientific reasoning. In addition, many companies are now investing in the development of general-purpose AI 'agents' – systems which can autonomously plan and act to achieve goals with little or no human oversight.

Building on the Interim Report (May 2024), the present report reflects these new developments. In addition, the experts contributing to this report made several other changes compared to the Interim Report. For example, they worked to further improve the scientific rigour of all sections, added discussion of additional topics such as open-weight models, and restructured the report to be more relevant to policymakers, including by highlighting evidence gaps and key challenges for policymakers.

I extend my profound gratitude to the team of experts who contributed to this report, including our writers, senior advisers, and the international Expert Advisory Panel. I have been impressed with their scientific excellence and expertise as well as the collaborative attitude with which they have approached this challenging project. I am also grateful to the industry and civil society organisations who reviewed the report, contributing invaluable feedback that has led this report to be more comprehensive than it otherwise would have been.

My thanks also go to the UK Government for starting this process and offering outstanding operational support. It was also important for me that the UK Government agreed that the scientists writing this report should have complete independence.

AI remains a fast-moving field. To keep up with this pace, policymakers and governments need to have access to the current scientific understanding on what risks advanced AI might pose. I hope that this report as well as future publications will help decision-makers ensure that people around the world can reap the benefits of AI safely.

### Taking advantage of AI opportunities safely calls for global collaboration
**Clara Chappaz, France's Minister Delegate for Artificial Intelligence**
**The Rt Hon Peter Kyle MP, UK Secretary of State for Science, Innovation and Technology**

Since the interim version of this report was published, the capabilities of advanced AI capabilities have continued to grow. We know that this technology, if developed and utilised safely and responsibly, offers extraordinary opportunities: to grow our economies, modernise our public services, and improve lives for our people. To seize these opportunities, it is imperative that we deepen our collective understanding of how AI can be developed safely.

This landmark report is testament to the value of global cooperation in forging this shared understanding. It is the result of over 90 AI experts from different continents, sectors, and areas of expertise, coming together to offer leaders and decision-makers a global reference point and a tool to inform policy on AI safety. Our collective understanding of frontier AI systems has improved. However, this report highlights that frontier AI remains a field of active scientific inquiry, with experts continuing to disagree on its trajectory and the scope of its impact.

We will maintain the momentum behind this collective effort to drive global scientific consensus. We are excited to continue this unprecedented and essential project of international collaboration. The report lays the foundation for important discussions at the AI Action Summit in France this year, which will convene international governments, leading AI companies, civil society groups and experts. This Summit, like the report, is a continuation of the milestones achieved at the Bletchley Park (November 2023) and Seoul (May 2024) summits. AI is the defining opportunity of our generation. Together, we will continue the conversation and support bold and ambitious action to collectively master the risks of AI and benefit from these new technologies for the greater good. There will be no adoption of this technology without safety: safety brings trust!

We are pleased to present this report and thank Professor Yoshua Bengio and the writing team for the significant work that went into its development. The UK and France look forward to continuing the discussion at the AI Action Summit in February.

---

## About this report

● This is the first International AI Safety Report. Following an interim publication in May 2024, a diverse group of 96 Artificial Intelligence (AI) experts contributed to this first full report, including an international Expert Advisory Panel nominated by 30 countries, the Organisation for Economic Co-operation and Development (OECD), the European Union (EU), and the United Nations (UN). The report aims to provide scientific information that will support informed policymaking. It does not recommend specific policies.

● The report is the work of independent experts. Led by the Chair, the independent experts writing this report collectively had full discretion over its content.

● While this report is concerned with AI risks and AI safety, AI also offers many potential benefits for people, businesses, and society. There are many types of AI, each with different benefits and risks. Most of the time, in most applications, AI helps individuals and organisations be more effective. But people around the world will only be able to fully enjoy AI's many potential benefits safely if its risks are appropriately managed. This report focuses on identifying these risks and evaluating methods for mitigating them. It does not aim to comprehensively assess all possible societal impacts of AI, including its many potential benefits.

● The focus of the report is general-purpose AI. The report restricts its focus to a type of AI that has advanced particularly rapidly in recent years, and whose associated risks have been less studied and understood: general-purpose AI, or AI that can perform a wide variety of tasks. The analysis in this report focuses on the most advanced general-purpose AI systems at the time of writing, as well as future systems that might be even more capable.

● The report summarises the scientific evidence on three core questions: What can general-purpose AI do? What are risks associated with general-purpose AI? And what mitigation techniques are there against these risks?

● The stakes are high. We, the experts contributing to this report, continue to disagree on several questions, minor and major, around general-purpose AI capabilities, risks, and risk mitigations. But we consider this report essential for improving our collective understanding of this technology and its potential risks. We hope that the report will help the international community to move towards greater consensus about general-purpose AI and mitigate its risks more effectively, so that people can safely experience its many potential benefits. The stakes are high. We look forward to continuing this effort.

---

## Update on latest AI advances after the writing of this report: Chair's note

Between the end of the writing period for this report (5 December 2024) and the publication of this report in January 2025, an important development took place. The AI company OpenAI shared early test results from a new AI model, **o3**. These results indicate significantly stronger performance than any previous model on a number of the field's most challenging tests of programming, abstract reasoning, and scientific reasoning. In some of these tests, o3 outperforms many (but not all) human experts. Additionally, it achieves a breakthrough on a key abstract reasoning test that many experts, including myself, thought was out of reach until recently. However, at the time of writing there is no public information about its real-world capabilities, particularly for solving more open-ended tasks.

### o3 Performance on Key Benchmarks

**Figure 0.1**: Scores of notable general-purpose AI models on key benchmarks from June 2023 to December 2024. o3 showed significantly improved performance compared to the previous state of the art (shaded region). These benchmarks are some of the field's most challenging tests of programming, abstract reasoning, and scientific reasoning.

Benchmarks tested:
- **GPQA**: Graduate-level science
- **SWE-bench**: Real-world software engineering
- **ARC-AGI**: Abstract reasoning (semi-secret evaluation)
- **AIME 2024**: Mathematics competition for elite students
- **FrontierMath**: Advanced mathematics

For the unreleased o3, the announcement date is shown; for the other models, the release date is shown. Some of the more recent AI models, including o3, benefited from improved scaffolding and more computation at test-time.

*Sources: Anthropic, 2024; Chollet, 2024; Chollet et al., 2025; Epoch AI, 2024; Glazer et al. 2024; OpenAI, 2024a; OpenAI, 2024b; Jimenez et al., 2024; Jimenez et al., 2025.*

### Implications of o3

The o3 results are evidence that the pace of advances in AI capabilities may remain high or even accelerate. More specifically, they suggest that giving models more computing power for solving a given problem ('**inference scaling**') may help overcome previous limitations. Generally speaking, inference scaling makes models more expensive to use. But as another recent notable model, **R1**, released by the company DeepSeek in January 2025, has shown, researchers are successfully working on lowering these costs. Overall, inference scaling may allow AI developers to make further advances going forward. The o3 results also underscore the need to better understand how AI developers' growing use of AI may affect the speed of further AI development itself.

The trends evidenced by o3 could have profound implications for AI risks. Advances in science and programming capabilities have previously generated more evidence for risks such as cyber and biological attacks. The o3 results are also relevant to potential labour market impacts, loss of control risk, and energy use among others. But o3's capabilities could also be used to help protect against malfunctions and malicious uses. Overall, the risk assessments in this report should be read with the understanding that AI has gained capabilities since the report was written. However, so far there is no evidence yet about o3's real world impacts, and no information to confirm nor rule out major novel and/or immediate risks.

The improvement in capabilities suggested by the o3 results and our limited understanding of the implications for AI risks underscore a key challenge for policymakers that this report identifies: they will often have to weigh potential benefits and risks of imminent AI advancements without having a large body of scientific evidence available. Nonetheless, generating evidence on the safety and security implications of the trends implied by o3 will be an urgent priority for AI research in the coming weeks and months.

---

## Key findings of the report

● The capabilities of general-purpose AI, the type of AI that this report focuses on, have increased rapidly in recent years and have improved further in recent months.† A few years ago, the best large language models (LLMs) could rarely produce a coherent paragraph of text. Today, general-purpose AI can write computer programs, generate custom photorealistic images, and engage in extended open-ended conversations. Since the publication of the Interim Report (May 2024), new models have shown markedly better performance at tests of scientific reasoning and programming.

● Many companies are now investing in the development of general-purpose AI agents, as a potential direction for further advancement. AI agents are general-purpose AI systems which can autonomously act, plan, and delegate to achieve goals with little to no human oversight. Sophisticated AI agents would be able to, for example, use computers to complete longer projects than current systems, unlocking both additional benefits and additional risks.

● Further capability advancements in the coming months and years could be anything from slow to extremely rapid.† Progress will depend on whether companies will be able to rapidly deploy even more data and computational power to train new models, and whether 'scaling' models in this way will overcome their current limitations. Recent research suggests that rapidly scaling up models may remain physically feasible for at least several years. But major capability advances may also require other factors: for example, new research breakthroughs, which are hard to predict, or the success of a novel scaling approach that companies have recently adopted.

● Several harms from general-purpose AI are already well established. These include scams, non-consensual intimate imagery (NCII) and child sexual abuse material (CSAM), model outputs that are biased against certain groups of people or certain opinions, reliability issues, and privacy violations. Researchers have developed mitigation techniques for these problems, but so far no combination of techniques can fully resolve them. Since the publication of the Interim Report, new evidence of discrimination related to general-purpose AI systems has revealed more subtle forms of bias.

● As general-purpose AI becomes more capable, evidence of additional risks is gradually emerging. These include risks such as large-scale labour market impacts, AI-enabled hacking or biological attacks, and society losing control over general-purpose AI. Experts interpret the existing evidence on these risks differently: some think that such risks are decades away, while others think that general-purpose AI could lead to societal-scale harm within the next few years. Recent advances in general-purpose AI capabilities – particularly in tests of scientific reasoning and programming – have generated new evidence for potential risks such as AI-enabled hacking and biological attacks, leading one major AI company to increase its assessment of biological risk from its best model from 'low' to 'medium'.

● Risk management techniques are nascent, but progress is possible. There are various technical methods to assess and reduce risks from general-purpose AI that developers can employ and regulators can require, but they all have limitations. For example, current interpretability techniques for explaining why a general-purpose AI model produced any given output remain severely limited. However, researchers are making some progress in addressing these limitations. In addition, researchers and policymakers are increasingly trying to standardise risk management approaches, and to coordinate internationally.

● The pace and unpredictability of advancements in general-purpose AI pose an '**evidence dilemma**' for policymakers.† Given sometimes rapid and unexpected advancements, policymakers will often have to weigh potential benefits and risks of imminent AI advancements without having a large body of scientific evidence available. In doing so, they face a dilemma. On the one hand, pre-emptive risk mitigation measures based on limited evidence might turn out to be ineffective or unnecessary. On the other hand, waiting for stronger evidence of impending risk could leave society unprepared or even make mitigation impossible – for instance if sudden leaps in AI capabilities, and their associated risks, occur. Companies and governments are developing early warning systems and risk management frameworks that may reduce this dilemma. Some of these trigger specific mitigation measures when there is new evidence of risks, while others require developers to provide evidence of safety before releasing a new model.

● There is broad consensus among researchers that advances regarding the following questions would be helpful: How rapidly will general-purpose AI capabilities advance in the coming years, and how can researchers reliably measure that progress? What are sensible risk thresholds to trigger mitigations? How can policymakers best gain access to information about general-purpose AI that is relevant to public safety? How can researchers, technology companies, and governments reliably assess the risks of general-purpose AI development and deployment? How do general-purpose AI models work internally? How can general-purpose AI be designed to behave reliably?

● **AI does not happen to us: choices made by people determine its future.** The future of general-purpose AI technology is uncertain, with a wide range of trajectories appearing to be possible even in the near future, including both very positive and very negative outcomes. This uncertainty can evoke fatalism and make AI appear as something that happens to us. But it will be the decisions of societies and governments on how to navigate this uncertainty that determine which path we will take. This report aims to facilitate constructive and evidence-based discussion about these decisions.

† Please refer to the Chair's update on the latest AI advances after the writing of this report.
