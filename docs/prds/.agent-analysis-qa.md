# QA Expert Analysis Request

**qa-expert:** Analyze FLUO's testing and quality infrastructure gaps for production readiness.

## Analysis Required

### 1. Test Coverage Audit
**Backend:**
- Review backend/src/test/java/com/fluo/ - what has tests?
- Check test coverage (mvn jacoco:report results)
- Identify critical paths without tests
- Review backend/ERROR_MESSAGES_GUIDE.md for error testing

**Frontend:**
- Review bff/src/ for test files
- Check for unit tests, integration tests, E2E tests
- Vitest configuration and coverage

### 2. Testing Infrastructure
**What exists:**
- [ ] Unit test framework (JUnit 5 backend, Vitest frontend?)
- [ ] Integration tests (API tests, component tests?)
- [ ] E2E tests (Playwright mentioned - exists?)
- [ ] Performance tests (benchmarks in backend/src/test/java/com/fluo/benchmarks/?)
- [ ] Load testing (can system handle production load?)
- [ ] Chaos testing (resilience to failures?)

### 3. Quality Gates
**What's enforced:**
- [ ] Test coverage thresholds (90% instruction, 80% branch?)
- [ ] Mutation testing (>70% mentioned in agent instructions)
- [ ] Static analysis (linters, type checking?)
- [ ] Security scanning (OWASP, dependency scanning?)
- [ ] Performance regression detection
- [ ] CI/CD pipeline (where? GitHub Actions?)

### 4. Critical Test Scenarios Missing
**DSL Parser & Rules:**
- Malicious DSL input (injection attacks?)
- Invalid rule syntax edge cases
- Drools compilation errors
- Rule engine memory limits
- Concurrent rule evaluation

**Multi-Tenancy:**
- Tenant isolation verification tests
- Cross-tenant data leak prevention
- Tenant session cleanup

**Compliance:**
- PII redaction validation tests
- Compliance span integrity tests
- Missing compliance evidence detection

**Performance:**
- High trace volume (1M+ spans/sec?)
- Long-lived traces (thousands of spans?)
- Drools session memory usage
- Rule engine throughput

### 5. Production Readiness Gaps
- [ ] Health checks and readiness probes
- [ ] Graceful shutdown
- [ ] Error handling and recovery
- [ ] Logging and observability
- [ ] Metrics and alerting
- [ ] Backup and recovery
- [ ] Disaster recovery plan

## Output Format

For each testing gap:
1. **Gap Title** - What's not tested
2. **Current State** - What tests exist (file paths)
3. **Risk** - What could go wrong without this
4. **Required Tests** - What needs to be built
5. **Priority** - P0/P1/P2 based on production risk
6. **Complexity** - Simple/Medium/Complex
7. **Dependencies** - What must exist first

Prioritize by production risk - P0 gaps that could cause outages or data loss first.
